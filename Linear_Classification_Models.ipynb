{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62274576",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "id": "1e54267c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T17:41:33.829656Z",
     "start_time": "2025-11-02T17:41:33.825269Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import MNIST\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from Neural_Networks import epochs, training_accurices, validation_accurices"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "faa13923",
   "metadata": {},
   "source": [
    "# 1. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a8ff6e",
   "metadata": {},
   "source": [
    "### 1.1. Read data from file"
   ]
  },
  {
   "cell_type": "code",
   "id": "f505c9c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T17:41:33.960524Z",
     "start_time": "2025-11-02T17:41:33.882192Z"
    }
   },
   "source": [
    "#load the datasets (train data and test data which are pre-splitted)\n",
    "data1 = MNIST(root='MNIST_dataset', train=True, download=True)\n",
    "data2 = MNIST(root='MNIST_dataset', train=False, download=True)\n",
    "\n",
    "#concatenate the data and labels from train and test datasets\n",
    "all_images = torch.cat((data1.data, data2.data), dim=0)\n",
    "all_labels = torch.cat((data1.targets, data2.targets), dim=0)"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "id": "e3c8c1d3",
   "metadata": {},
   "source": [
    "Now, we have 70,000 images intotal."
   ]
  },
  {
   "cell_type": "code",
   "id": "555a1328",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T17:41:34.055335Z",
     "start_time": "2025-11-02T17:41:34.005110Z"
    }
   },
   "source": [
    "x = all_images.numpy()\n",
    "y = all_labels.numpy()\n",
    "\n",
    "print(\"Total images:\", x.shape[0])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 70000\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "dbdc43d1",
   "metadata": {},
   "source": [
    "### 1.2. Normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "id": "d8c0cab0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T17:41:34.353251Z",
     "start_time": "2025-11-02T17:41:34.111344Z"
    }
   },
   "source": [
    "x = x / 255.0  # Normalize pixel values to [0, 1] range\n",
    "\n",
    "print(\"min pixel value:\", x.min(), \", max pixel value:\", x.max())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min pixel value: 0.0 , max pixel value: 1.0\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "55f33f71",
   "metadata": {},
   "source": [
    "### 1.3. Splitting into train, validtaion and test sets"
   ]
  },
  {
   "cell_type": "code",
   "id": "5c5a851e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T17:41:34.691135Z",
     "start_time": "2025-11-02T17:41:34.404695Z"
    }
   },
   "source": [
    "x_train, x_rest, y_train, y_rest = train_test_split(x, y, train_size= 0.6, random_state=42, stratify=y) # 60% train, 40% to split again into val and test\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_rest, y_rest, train_size=0.5, random_state=42, stratify=y_rest) # 40% * 50% = 20% val, 20% test\n",
    "\n",
    "print(\"Training set = \", x_train.shape[0])\n",
    "print(\"Validation set = \", x_val.shape[0])\n",
    "print(\"Test set = \", x_test.shape[0])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set =  42000\n",
      "Validation set =  14000\n",
      "Test set =  14000\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "66350be9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T17:41:34.795997Z",
     "start_time": "2025-11-02T17:41:34.742867Z"
    }
   },
   "source": [
    "x_train_flat = x_train.reshape(x_train.shape[0], -1)\n",
    "x_val_flat = x_val.reshape(x_val.shape[0], -1)\n",
    "x_test_flat = x_test.reshape(x_test.shape[0], -1)\n",
    "\n",
    "# Verify the new shape\n",
    "print(f\"Original shape: {x_train.shape}\")\n",
    "print(f\"Flattened shape: {x_train_flat.shape}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (42000, 28, 28)\n",
      "Flattened shape: (42000, 784)\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "af943971",
   "metadata": {},
   "source": [
    "### 1.4. Pytorch DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "id": "453913ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T17:41:34.931926Z",
     "start_time": "2025-11-02T17:41:34.855897Z"
    }
   },
   "source": [
    "# Change from numpy arrays to tensors\n",
    "x_train_tensor = torch.from_numpy(x_train_flat).unsqueeze(1).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).long()\n",
    "\n",
    "x_val_tensor = torch.from_numpy(x_val_flat).unsqueeze(1).float()\n",
    "y_val_tensor = torch.from_numpy(y_val).long()\n",
    "\n",
    "x_test_tensor = torch.from_numpy(x_test_flat).unsqueeze(1).float()\n",
    "y_test_tensor = torch.from_numpy(y_test).long()\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(x_train_tensor, y_train_tensor)\n",
    "val_dataset = torch.utils.data.TensorDataset(x_val_tensor, y_val_tensor)\n",
    "test_dataset = torch.utils.data.TensorDataset(x_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders with batch size = 64 to optimize training\n",
    "train_NN_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_NN_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_NN_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "13177691",
   "metadata": {},
   "source": [
    "# 2. Linear Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edcb0ae",
   "metadata": {},
   "source": [
    "## 2.1. Logistic Regression\n",
    "\n",
    "For logistic regression, it's a binary classifier, so we need to use two digits only as classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec25e45",
   "metadata": {},
   "source": [
    "### 2.1.1. Data preparation For Binary Classification\n",
    "\n",
    "Boolean masking is used to filter the dataset for two specific digits."
   ]
  },
  {
   "cell_type": "code",
   "id": "df0d620f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T17:41:34.985767Z",
     "start_time": "2025-11-02T17:41:34.981256Z"
    }
   },
   "source": [
    "def filter_digits(x, y, digit1, digit2):\n",
    "    filter_mask = (y == digit1) | (y == digit2)\n",
    "\n",
    "    x_filtered = x[filter_mask]\n",
    "    y_filtered = y[filter_mask]\n",
    "\n",
    "    y_filtered = np.where(y_filtered == digit1, 0, 1) # Map digit1 to 0 and digit2 to 1\n",
    "    return x_filtered, y_filtered"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "id": "aad530c3",
   "metadata": {},
   "source": [
    "Filter the data for only 2 digits."
   ]
  },
  {
   "cell_type": "code",
   "id": "066a878c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T17:41:35.106721Z",
     "start_time": "2025-11-02T17:41:35.054176Z"
    }
   },
   "source": [
    "DIGIT_A = 0\n",
    "DIGIT_B = 1\n",
    "\n",
    "x_binary_train_flat, y_binary_train = filter_digits(x_train_flat, y_train, DIGIT_A, DIGIT_B)\n",
    "x_binary_val_flat, y_binary_val = filter_digits(x_val_flat, y_val, DIGIT_A, DIGIT_B)\n",
    "x_binary_test_flat, y_binary_test = filter_digits(x_test_flat, y_test, DIGIT_A, DIGIT_B)"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "a60387eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T17:41:35.168448Z",
     "start_time": "2025-11-02T17:41:35.161523Z"
    }
   },
   "source": [
    "print(x_binary_train_flat.shape, y_binary_train.shape)\n",
    "print(x_binary_val_flat.shape, y_binary_val.shape)\n",
    "print(x_binary_test_flat.shape, y_binary_test.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8868, 784) (8868,)\n",
      "(2957, 784) (2957,)\n",
      "(2955, 784) (2955,)\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "e74f1ce8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T17:41:35.249429Z",
     "start_time": "2025-11-02T17:41:35.227383Z"
    }
   },
   "source": [
    "# Convert to float tensors\n",
    "# Convert labels to columns vector\n",
    "x_train_binary_tensor = torch.from_numpy(x_binary_train_flat).float()\n",
    "y_train_binary_tensor = torch.from_numpy(y_binary_train).float().view(-1, 1)\n",
    "\n",
    "x_val_binary_tensor = torch.from_numpy(x_binary_val_flat).float()\n",
    "y_val_binary_tensor = torch.from_numpy(y_binary_val).float().view(-1, 1)\n",
    "\n",
    "x_test_binary_tensor = torch.from_numpy(x_binary_test_flat).float()\n",
    "y_test_binary_tensor = torch.from_numpy(y_binary_test).float().view(-1, 1)\n",
    "\n",
    "train_binary_dataset = torch.utils.data.TensorDataset(x_train_binary_tensor, y_train_binary_tensor)\n",
    "val_binary_dataset = torch.utils.data.TensorDataset(x_val_binary_tensor, y_val_binary_tensor)\n",
    "test_binary_dataset = torch.utils.data.TensorDataset(x_test_binary_tensor, y_test_binary_tensor)\n",
    "\n",
    "train_lr_loader = DataLoader(train_binary_dataset, batch_size=64, shuffle=True)\n",
    "val_lr_loader = DataLoader(val_binary_dataset, batch_size=64, shuffle=False)\n",
    "test_lr_loader = DataLoader(test_binary_dataset, batch_size=64, shuffle=False)"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "id": "23a1c8db",
   "metadata": {},
   "source": [
    "### 2.1.2. Defining The Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "2fb89650",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T17:41:35.330461Z",
     "start_time": "2025-11-02T17:41:35.325736Z"
    }
   },
   "source": [
    "INPUT_FEATURES = 784 # Number of pixels/inputs (28x28)\n",
    "OUTPUT_FEATURES = 1  # The output is either 0 or 1\n",
    "\n",
    "# Create the weights tensor of random numbers with size of (784, 1)\n",
    "weights = torch.randn(INPUT_FEATURES, OUTPUT_FEATURES, dtype=torch.float)\n",
    "\n",
    "# Create the bias tensor of zeros with size of 1 element\n",
    "bias = torch.zeros(OUTPUT_FEATURES, dtype=torch.float)\n",
    "\n",
    "# Enable gradient tracking for weights and bias\n",
    "weights.requires_grad = True\n",
    "bias.requires_grad = True"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "id": "115801e1",
   "metadata": {},
   "source": [
    "Define Sigmoid activation function and the cross entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "id": "f14cd034",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T17:41:35.417920Z",
     "start_time": "2025-11-02T17:41:35.411901Z"
    }
   },
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + torch.exp(-z))\n",
    "\n",
    "# y_true are the labels (0 or 1) and y_pred_logits are the model outputs (logits)\n",
    "def binary_cross_entropy_loss(y_true, y_pred_logits):\n",
    "\n",
    "    p = sigmoid(y_pred_logits)\n",
    "    \n",
    "    # Prevent log(0) by adding a small value (epsilon)\n",
    "    epsilon = 1e-7\n",
    "\n",
    "    loss_per_item = - ( y_true * torch.log(p + epsilon) + (1.0 - y_true) * torch.log(1.0 - p + epsilon) )\n",
    "\n",
    "    # Return average\n",
    "    return torch.mean(loss_per_item)"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "id": "1decc622",
   "metadata": {},
   "source": [
    "### 2.1.3 Training The Model"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T17:41:44.232147Z",
     "start_time": "2025-11-02T17:41:35.491043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 50\n",
    "\n",
    "train_losses, val_losses, train_accuracies, val_accuracies = [], [], [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    correct, total, epoch_loss = 0,0,0\n",
    "\n",
    "    for x_batch, y_batch in train_lr_loader:\n",
    "        # Forward Pass\n",
    "        logits = x_batch @ weights + bias\n",
    "\n",
    "        loss = binary_cross_entropy_loss(y_batch, logits)\n",
    "        loss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            weights -= learning_rate * weights.grad\n",
    "            bias -= learning_rate * bias.grad\n",
    "\n",
    "        # Reset Gradients\n",
    "        weights.grad.zero_()\n",
    "        bias.grad.zero_()\n",
    "\n",
    "        # Track Loss\n",
    "        epoch_loss += loss.item() * x_batch.size(0)\n",
    "\n",
    "        # Compute Accuracy\n",
    "        predictions = (sigmoid(logits) >= 0.5).float()\n",
    "        correct += (predictions == y_batch).sum().item()\n",
    "        total += x_batch.size(0)\n",
    "\n",
    "    train_losses.append(epoch_loss / total)\n",
    "    train_accuracies.append(correct / total)\n",
    "\n",
    "    # Validation\n",
    "    with torch.no_grad():\n",
    "        val_logits = x_val_binary_tensor @ weights + bias\n",
    "        val_predictions = sigmoid(val_logits)\n",
    "        val_loss = binary_cross_entropy_loss(y_val_binary_tensor, val_logits)\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "        val_pred_labels = (val_predictions >= 0.5).float()\n",
    "        val_accuracy = (val_pred_labels == y_val_binary_tensor).float().mean().item()\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "          f\"Train Loss: {train_losses[-1]:.4f} | Train Acc: {train_accuracies[-1]*100:.2f}% | \"\n",
    "          f\"Val Acc: {val_accuracies[-1]*100:.2f}%\")"
   ],
   "id": "ab26b733c396a1bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 1.9238 | Train Acc: 59.18% | Val Acc: 78.69%\n",
      "Epoch 2/50 | Train Loss: 0.4878 | Train Acc: 85.46% | Val Acc: 90.23%\n",
      "Epoch 3/50 | Train Loss: 0.2472 | Train Acc: 92.21% | Val Acc: 93.85%\n",
      "Epoch 4/50 | Train Loss: 0.1661 | Train Acc: 94.65% | Val Acc: 95.71%\n",
      "Epoch 5/50 | Train Loss: 0.1274 | Train Acc: 96.11% | Val Acc: 96.28%\n",
      "Epoch 6/50 | Train Loss: 0.1051 | Train Acc: 96.85% | Val Acc: 96.75%\n",
      "Epoch 7/50 | Train Loss: 0.0905 | Train Acc: 97.26% | Val Acc: 97.23%\n",
      "Epoch 8/50 | Train Loss: 0.0801 | Train Acc: 97.72% | Val Acc: 97.50%\n",
      "Epoch 9/50 | Train Loss: 0.0723 | Train Acc: 97.87% | Val Acc: 97.77%\n",
      "Epoch 10/50 | Train Loss: 0.0663 | Train Acc: 98.07% | Val Acc: 97.84%\n",
      "Epoch 11/50 | Train Loss: 0.0614 | Train Acc: 98.22% | Val Acc: 98.04%\n",
      "Epoch 12/50 | Train Loss: 0.0574 | Train Acc: 98.27% | Val Acc: 98.14%\n",
      "Epoch 13/50 | Train Loss: 0.0540 | Train Acc: 98.41% | Val Acc: 98.21%\n",
      "Epoch 14/50 | Train Loss: 0.0512 | Train Acc: 98.48% | Val Acc: 98.28%\n",
      "Epoch 15/50 | Train Loss: 0.0487 | Train Acc: 98.53% | Val Acc: 98.38%\n",
      "Epoch 16/50 | Train Loss: 0.0465 | Train Acc: 98.64% | Val Acc: 98.41%\n",
      "Epoch 17/50 | Train Loss: 0.0446 | Train Acc: 98.67% | Val Acc: 98.48%\n",
      "Epoch 18/50 | Train Loss: 0.0428 | Train Acc: 98.73% | Val Acc: 98.51%\n",
      "Epoch 19/50 | Train Loss: 0.0413 | Train Acc: 98.80% | Val Acc: 98.68%\n",
      "Epoch 20/50 | Train Loss: 0.0399 | Train Acc: 98.88% | Val Acc: 98.78%\n",
      "Epoch 21/50 | Train Loss: 0.0386 | Train Acc: 98.92% | Val Acc: 98.88%\n",
      "Epoch 22/50 | Train Loss: 0.0374 | Train Acc: 98.95% | Val Acc: 98.92%\n",
      "Epoch 23/50 | Train Loss: 0.0363 | Train Acc: 98.96% | Val Acc: 98.92%\n",
      "Epoch 24/50 | Train Loss: 0.0353 | Train Acc: 98.99% | Val Acc: 98.92%\n",
      "Epoch 25/50 | Train Loss: 0.0344 | Train Acc: 98.99% | Val Acc: 98.92%\n",
      "Epoch 26/50 | Train Loss: 0.0335 | Train Acc: 99.01% | Val Acc: 99.02%\n",
      "Epoch 27/50 | Train Loss: 0.0327 | Train Acc: 99.01% | Val Acc: 99.05%\n",
      "Epoch 28/50 | Train Loss: 0.0320 | Train Acc: 99.04% | Val Acc: 99.05%\n",
      "Epoch 29/50 | Train Loss: 0.0312 | Train Acc: 99.09% | Val Acc: 99.09%\n",
      "Epoch 30/50 | Train Loss: 0.0306 | Train Acc: 99.13% | Val Acc: 99.09%\n",
      "Epoch 31/50 | Train Loss: 0.0299 | Train Acc: 99.13% | Val Acc: 99.09%\n",
      "Epoch 32/50 | Train Loss: 0.0293 | Train Acc: 99.15% | Val Acc: 99.12%\n",
      "Epoch 33/50 | Train Loss: 0.0288 | Train Acc: 99.17% | Val Acc: 99.12%\n",
      "Epoch 34/50 | Train Loss: 0.0282 | Train Acc: 99.19% | Val Acc: 99.12%\n",
      "Epoch 35/50 | Train Loss: 0.0277 | Train Acc: 99.19% | Val Acc: 99.15%\n",
      "Epoch 36/50 | Train Loss: 0.0272 | Train Acc: 99.20% | Val Acc: 99.15%\n",
      "Epoch 37/50 | Train Loss: 0.0267 | Train Acc: 99.21% | Val Acc: 99.19%\n",
      "Epoch 38/50 | Train Loss: 0.0263 | Train Acc: 99.22% | Val Acc: 99.22%\n",
      "Epoch 39/50 | Train Loss: 0.0258 | Train Acc: 99.23% | Val Acc: 99.22%\n",
      "Epoch 40/50 | Train Loss: 0.0254 | Train Acc: 99.24% | Val Acc: 99.22%\n",
      "Epoch 41/50 | Train Loss: 0.0250 | Train Acc: 99.27% | Val Acc: 99.22%\n",
      "Epoch 42/50 | Train Loss: 0.0246 | Train Acc: 99.27% | Val Acc: 99.22%\n",
      "Epoch 43/50 | Train Loss: 0.0242 | Train Acc: 99.29% | Val Acc: 99.26%\n",
      "Epoch 44/50 | Train Loss: 0.0239 | Train Acc: 99.29% | Val Acc: 99.26%\n",
      "Epoch 45/50 | Train Loss: 0.0235 | Train Acc: 99.30% | Val Acc: 99.26%\n",
      "Epoch 46/50 | Train Loss: 0.0232 | Train Acc: 99.31% | Val Acc: 99.26%\n",
      "Epoch 47/50 | Train Loss: 0.0229 | Train Acc: 99.31% | Val Acc: 99.26%\n",
      "Epoch 48/50 | Train Loss: 0.0226 | Train Acc: 99.31% | Val Acc: 99.26%\n",
      "Epoch 49/50 | Train Loss: 0.0223 | Train Acc: 99.31% | Val Acc: 99.26%\n",
      "Epoch 50/50 | Train Loss: 0.0220 | Train Acc: 99.35% | Val Acc: 99.26%\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Add more sections before this and change numbering if needed",
   "id": "e3be303b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "27058835b6d9ea8d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
